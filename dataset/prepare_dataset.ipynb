{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/siddhantmedar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create directory\n",
    "Path('LitFactTechMix').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got disconnected from remote data host. Retrying in 5sec [1/20]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m wikitext_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100000\u001b[39m), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m     10\u001b[0m arxiv_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1700000\u001b[39m), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m pg19_sample \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpg19\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpg19_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m wikitext_sample \u001b[38;5;241m=\u001b[39m [entry \u001b[38;5;28;01mfor\u001b[39;00m i, entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(wikitext) \u001b[38;5;28;01mif\u001b[39;00m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m wikitext_indices]\n\u001b[1;32m     14\u001b[0m arxiv_sample \u001b[38;5;241m=\u001b[39m [entry \u001b[38;5;28;01mfor\u001b[39;00m i, entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arxiv) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m arxiv_indices]\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m wikitext_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100000\u001b[39m), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m     10\u001b[0m arxiv_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1700000\u001b[39m), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m pg19_sample \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpg19\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpg19_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m wikitext_sample \u001b[38;5;241m=\u001b[39m [entry \u001b[38;5;28;01mfor\u001b[39;00m i, entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(wikitext) \u001b[38;5;28;01mif\u001b[39;00m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m wikitext_indices]\n\u001b[1;32m     14\u001b[0m arxiv_sample \u001b[38;5;241m=\u001b[39m [entry \u001b[38;5;28;01mfor\u001b[39;00m i, entry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arxiv) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m arxiv_indices]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/datasets/iterable_dataset.py:2270\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2270\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[1;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/datasets/iterable_dataset.py:1856\u001b[0m, in \u001b[0;36mFormattedExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1849\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m get_formatter(\n\u001b[1;32m   1850\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatting\u001b[38;5;241m.\u001b[39mformat_type,\n\u001b[1;32m   1851\u001b[0m         features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable\u001b[38;5;241m.\u001b[39mis_typed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1852\u001b[0m         token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_per_repo_id,\n\u001b[1;32m   1853\u001b[0m     )\n\u001b[1;32m   1854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable\u001b[38;5;241m.\u001b[39miter_arrow:\n\u001b[1;32m   1855\u001b[0m     \u001b[38;5;66;03m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[39;00m\n\u001b[0;32m-> 1856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_batch_to_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/datasets/iterable_dataset.py:1879\u001b[0m, in \u001b[0;36mFormattedExamplesIterable._iter_arrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable\u001b[38;5;241m.\u001b[39m_iter_arrow()\n\u001b[0;32m-> 1879\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mex_iterable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrow_schema\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/datasets/iterable_dataset.py:476\u001b[0m, in \u001b[0;36mRebatchedArrowExamplesIterable._iter_arrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m     previous_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m previous_state\n\u001b[0;32m--> 476\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_chunks_since_previous_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_chunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_chunks_to_skip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/datasets/iterable_dataset.py:128\u001b[0m, in \u001b[0;36m_convert_to_arrow\u001b[0;34m(iterable, batch_size, drop_last_batch)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    127\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterable)\n\u001b[0;32m--> 128\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterator_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_examples_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/datasets/iterable_dataset.py:222\u001b[0m, in \u001b[0;36mExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gen_kwags \u001b[38;5;129;01min\u001b[39;00m islice(_split_gen_kwargs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, max_num_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_shards), shard_idx_start, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    221\u001b[0m     shard_example_idx_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshard_example_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 222\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_examples_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_example_idx_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_dict\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshard_example_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/deepmind--pg19/fb74320038a3c19e3cc87375222fc75ed3c8dc5a739b3e8dc835736388a7a882/pg19.py:133\u001b[0m, in \u001b[0;36mPg19._generate_examples\u001b[0;34m(self, ids, metadata, files)\u001b[0m\n\u001b[1;32m    130\u001b[0m data \u001b[38;5;241m=\u001b[39m id2metadata[_id]\n\u001b[1;32m    131\u001b[0m file \u001b[38;5;241m=\u001b[39m files[_id]\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    134\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    136\u001b[0m _id \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/datasets/streaming.py:75\u001b[0m, in \u001b[0;36mextend_module_for_streaming.<locals>.wrap_auth.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/datasets/utils/file_utils.py:949\u001b[0m, in \u001b[0;36mxopen\u001b[0;34m(file, mode, download_config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    947\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 949\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m \u001b[43mfsspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot seek streaming HTTP file\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/fsspec/core.py:147\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    The OpenFile object should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    instances persisting. You must, therefore, keep a reference to the OpenFile\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    during the life of the file-like it generates.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/fsspec/core.py:105\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_magic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/fsspec/spec.py:1303\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1303\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/fsspec/implementations/http.py:361\u001b[0m, in \u001b[0;36mHTTPFileSystem._open\u001b[0;34m(self, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masynchronous\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masynchronous\n\u001b[1;32m    360\u001b[0m kw\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 361\u001b[0m size \u001b[38;5;241m=\u001b[39m size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    362\u001b[0m session \u001b[38;5;241m=\u001b[39m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_session)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block_size \u001b[38;5;129;01mand\u001b[39;00m size:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/site-packages/fsspec/asyn.py:91\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ml-env/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load datasets with streaming\n",
    "pg19 = load_dataset('deepmind/pg19', split='train', streaming=True)\n",
    "wikitext = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train', streaming=True)\n",
    "arxiv = load_dataset('MaartenGr/arxiv_nlp', split='train', streaming=True)\n",
    "\n",
    "# Sample subsets\n",
    "random.seed(42)\n",
    "pg19_indices = random.sample(range(11000), k=100)\n",
    "wikitext_indices = random.sample(range(100000), k=2000)\n",
    "arxiv_indices = random.sample(range(1700000), k=1000)\n",
    "\n",
    "pg19_sample = [entry for i, entry in enumerate(pg19) if i in pg19_indices]\n",
    "wikitext_sample = [entry for i, entry in enumerate(wikitext) if entry['text'].strip() and i in wikitext_indices]\n",
    "arxiv_sample = [entry for i, entry in enumerate(arxiv) if i in arxiv_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning functions\n",
    "def clean_pg19(text):\n",
    "    start = text.find('*** START OF THIS PROJECT GUTENBERG EBOOK')\n",
    "    end = text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "    if start != -1 and end != -1:\n",
    "        text = text[start+len('*** START OF THIS PROJECT GUTENBERG EBOOK'):end]\n",
    "    return re.sub(r'\\n+', ' ', text.lower()).strip()\n",
    "\n",
    "def clean_wikitext(text):\n",
    "    text = re.sub(r'@\\S+@', '', text)\n",
    "    return re.sub(r'\\n+', ' ', text.lower()).strip()\n",
    "\n",
    "def clean_arxiv(text):\n",
    "    text = re.sub(r'\\\\{.*?\\\\}', '', text)\n",
    "    return re.sub(r'\\n+', ' ', text.lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract and clean texts\n",
    "pg19_texts = [clean_pg19(entry['text']) for entry in pg19_sample if entry['text']]\n",
    "wikitext_texts = [clean_wikitext(entry['text']) for entry in wikitext_sample if entry['text']]\n",
    "arxiv_texts = [clean_arxiv(entry['abstract']) for entry in arxiv_sample if entry['abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine and shuffle\n",
    "all_texts = pg19_texts + wikitext_texts + arxiv_texts\n",
    "random.shuffle(all_texts)\n",
    "\n",
    "# Tokenize with NLTK\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "all_words = []\n",
    "for text in all_texts:\n",
    "    all_words.extend(tokenize_text(text))\n",
    "vocab = sorted(set(all_words))\n",
    "vocab_size = len(vocab)\n",
    "wtoi = {w: i for i, w in enumerate(vocab)}\n",
    "itow = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "# Cap vocab at 50,000\n",
    "from collections import Counter\n",
    "word_counts = Counter(all_words)\n",
    "vocab = [w for w, _ in word_counts.most_common(50000)]\n",
    "vocab_size = len(vocab)\n",
    "wtoi = {w: i for i, w in enumerate(vocab)}\n",
    "itow = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "# Add <UNK>\n",
    "wtoi['<UNK>'] = len(vocab)\n",
    "itow[len(vocab)] = '<UNK>'\n",
    "vocab_size += 1\n",
    "\n",
    "def encode(text):\n",
    "    words = tokenize_text(text)\n",
    "    return [wtoi.get(w, wtoi['<UNK>']) for w in words]\n",
    "\n",
    "def decode(indices):\n",
    "    return ' '.join([itow.get(i, '<UNK>') for i in indices])\n",
    "\n",
    "# Tokenize dataset\n",
    "data = []\n",
    "for text in all_texts:\n",
    "    if text.strip():\n",
    "        data.extend(encode(text))\n",
    "data = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "# Split train/val\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Save dataset\n",
    "torch.save(train_data, 'custom_mini_mix/train.pt')\n",
    "torch.save(val_data, 'custom_mini_mix/val.pt')\n",
    "torch.save((wtoi, itow, vocab_size), 'custom_mini_mix/vocab.pt')\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"name\": \"custom_mini_mix\",\n",
    "    \"description\": \"A dataset for training language models, combining 100 books from PG-19, 2000 Wikipedia articles from WikiText-103, and 1000 NLP abstracts from ArXiv NLP. Preprocessed for word-based tokenization with a 50,000-word vocabulary plus <UNK>.\",\n",
    "    \"source_datasets\": [\n",
    "        {\"name\": \"deepmind/pg19\", \"license\": \"Apache 2.0\", \"size\": \"100 books (~20 MB)\"},\n",
    "        {\"name\": \"wikitext/wikitext-103-raw-v1\", \"license\": \"CC BY-SA 4.0\", \"size\": \"2000 articles (~15 MB)\"},\n",
    "        {\"name\": \"MaartenGr/arxiv_nlp\", \"license\": \"Not specified, academic use\", \"size\": \"1000 abstracts (~15 MB)\"}\n",
    "    ],\n",
    "    \"size\": \"Approx. 50 MB uncompressed, 15 MB compressed\",\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"splits\": {\"train\": len(train_data), \"validation\": len(val_data)},\n",
    "    \"usage\": \"Load train.pt, val.pt, and vocab.pt using torch.load. Use wtoi/itow for encoding/decoding.\",\n",
    "    \"license\": \"CC BY-SA 4.0\"\n",
    "}\n",
    "with open('custom_mini_mix/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Create README\n",
    "readme_content = \"\"\"---\n",
    "dataset_info:\n",
    "  dataset_name: custom_mini_mix\n",
    "  description: A dataset for training language models, combining 100 books from PG-19, 2000 Wikipedia articles from WikiText-103, and 1000 NLP abstracts from ArXiv NLP. Preprocessed for word-based tokenization with a 50,000-word vocabulary plus <UNK>.\n",
    "  license: CC BY-SA 4.0\n",
    "  size: Approx. 50 MB uncompressed, 15 MB compressed\n",
    "  splits:\n",
    "    train: {train_size} tokens\n",
    "    validation: {val_size} tokens\n",
    "  source_datasets:\n",
    "    - name: deepmind/pg19\n",
    "      license: Apache 2.0\n",
    "      size: 100 books (~20 MB)\n",
    "    - name: wikitext/wikitext-103-raw-v1\n",
    "      license: CC BY-SA 4.0\n",
    "      size: 2000 articles (~15 MB)\n",
    "    - name: MaartenGr/arxiv_nlp\n",
    "      license: Not specified, academic use\n",
    "      size: 1000 abstracts (~15 MB)\n",
    "  vocab_size: {vocab_size}\n",
    "---\n",
    "\n",
    "# Custom Mini Mix Dataset\n",
    "\n",
    "## Overview\n",
    "This dataset is designed for training small to medium language models, offering a diverse mix of literary, factual, and technical text. It combines:\n",
    "- **PG-19**: 100 books from classic literature for narrative and poetic styles.\n",
    "- **WikiText-103**: 2000 Wikipedia articles for factual, structured text.\n",
    "- **ArXiv NLP**: 1000 abstracts from NLP papers for technical, academic language.\n",
    "\n",
    "The dataset is preprocessed for word-based tokenization, with a vocabulary of 50,000 words plus an `<UNK>` token for unknown words. It is split into training ({train_size} tokens) and validation ({val_size} tokens) sets.\n",
    "\n",
    "## Files\n",
    "- `train.pt`: Training data (PyTorch tensor of word indices).\n",
    "- `val.pt`: Validation data (PyTorch tensor of word indices).\n",
    "- `vocab.pt`: Vocabulary (tuple of word-to-index and index-to-word dictionaries, vocab size).\n",
    "- `metadata.json`: Dataset metadata.\n",
    "\n",
    "## Usage\n",
    "Load the dataset in Python using PyTorch:\n",
    "```python\n",
    "import torch\n",
    "train_data = torch.load('train.pt')\n",
    "val_data = torch.load('val.pt')\n",
    "wtoi, itow, vocab_size = torch.load('vocab.pt')\n",
    "\n",
    "def decode(indices):\n",
    "    return ' '.join([itow.get(i, '<UNK>') for i in indices])\n",
    "```\n",
    "\n",
    "## Size\n",
    "- **Uncompressed**: ~50 MB\n",
    "- **Compressed**: ~15 MB\n",
    "- **Download Time**: ~1–2 minutes on a 10 Mbps connection\n",
    "- **Disk Usage**: ~50 MB (plus temporary space during training)\n",
    "\n",
    "## License\n",
    "Licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), inherited from WikiText-103 (most restrictive). Source datasets include Apache 2.0 (PG-19) and unspecified academic use (ArXiv NLP).\n",
    "\n",
    "## Source Datasets\n",
    "- [deepmind/pg19](https://huggingface.co/datasets/deepmind/pg19)\n",
    "- [wikitext/wikitext-103-raw-v1](https://huggingface.co/datasets/wikitext)\n",
    "- [MaartenGr/arxiv_nlp](https://huggingface.co/datasets/MaartenGr/arxiv_nlp)\n",
    "\n",
    "## Creation Process\n",
    "The dataset was created by:\n",
    "1. Sampling 100 books from PG-19, 2000 articles from WikiText-103, and 1000 abstracts from ArXiv NLP.\n",
    "2. Cleaning texts to remove boilerplate, markup, and LaTeX.\n",
    "3. Tokenizing with NLTK for word-based tokenization, capping vocabulary at 50,000 words.\n",
    "4. Saving as PyTorch tensors for training and validation.\n",
    "\n",
    "## Intended Use\n",
    "Ideal for training small language models (e.g., GPT-like architectures) for research or educational purposes. Suitable for generating literary, factual, or technical text.\n",
    "\n",
    "## Contact\n",
    "For issues or questions, open an issue on the [Hugging Face repository](https://huggingface.co/datasets/{repo_id}) or contact the creator.\n",
    "\n",
    "---\n",
    "Created on May 10, 2025.\n",
    "\"\"\"\n",
    "\n",
    "# Write README\n",
    "with open('custom_mini_mix/README.md', 'w') as f:\n",
    "    f.write(readme_content.format(\n",
    "        train_size=len(train_data),\n",
    "        val_size=len(val_data),\n",
    "        vocab_size=vocab_size,\n",
    "        repo_id=\"your_username/custom_mini_mix\"  # Replace with your username\n",
    "    ))\n",
    "\n",
    "# Create repository and upload\n",
    "repo_id = \"your_username/custom_mini_mix\"  # Replace with your username\n",
    "create_repo(repo_id, repo_type=\"dataset\", private=False)\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=\"custom_mini_mix\",\n",
    "    path_in_repo=\"\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
